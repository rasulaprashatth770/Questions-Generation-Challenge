{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl1dE064sLZ0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import faiss\n",
        "from datetime import datetime\n",
        "from PyPDF2 import PdfReader\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Set global parameters\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "GENERATION_MODEL = \"t5-base\"\n",
        "VECTOR_STORE = \"faiss_index\"\n",
        "OUTPUT_DIR = \"output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = []\n",
        "    for page in reader.pages:\n",
        "        text.append(page.extract_text())\n",
        "    return text\n",
        "\n",
        "# Function to create a vector store using FAISS\n",
        "def create_vector_store(text_chunks, embedding_model):\n",
        "    model = SentenceTransformer(embedding_model)\n",
        "    embeddings = model.encode(text_chunks, convert_to_tensor=False)\n",
        "    dimension = embeddings[0].shape[0]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(np.array(embeddings))\n",
        "    return index, embeddings\n",
        "\n",
        "# Function to retrieve relevant context from FAISS\n",
        "def retrieve_context(query, text_chunks, index, embedding_model, top_k=3):\n",
        "    model = SentenceTransformer(embedding_model)\n",
        "    query_embedding = model.encode([query], convert_to_tensor=False)\n",
        "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
        "    context = [text_chunks[i] for i in indices[0]]\n",
        "    return context, distances[0]\n",
        "\n",
        "# Function to extract main topics using T5\n",
        "def extract_topics_with_t5(text, model, tokenizer, num_topics=5):\n",
        "    prompt = f\"Extract {num_topics} main topics from the text:\\n{text[:512]}\"\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=150, num_beams=4, early_stopping=True)\n",
        "    topics = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return [topic.strip() for topic in topics.split(\"\\n\") if topic.strip()]\n",
        "\n",
        "# Function to generate MCQs using T5\n",
        "def generate_mcqs_with_context(topic, context, model, tokenizer, num_questions=10):\n",
        "    prompt = (\n",
        "        f\"Generate {num_questions} multiple-choice questions based on the topic '{topic}' \"\n",
        "        f\"using the following context:\\n{context}\\n\"\n",
        "        \"Provide each question with 4 options (A, B, C, D), specify the correct answer, and give a detailed explanation.\"\n",
        "    )\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=1024, num_beams=4, early_stopping=True)\n",
        "    mcqs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return mcqs\n",
        "\n",
        "# Main RAG pipeline\n",
        "def rag_pipeline(pdf_path, embedding_model, generation_model, vector_store_path):\n",
        "    book_title = \"Project Management Professional Guide\"\n",
        "\n",
        "    # Step 1: Extract text from PDF\n",
        "    pages = extract_text_from_pdf(pdf_path)\n",
        "    text_chunks = [page.strip() for page in pages if page.strip()]\n",
        "\n",
        "    # Step 2: Create FAISS vector store\n",
        "    if not os.path.exists(vector_store_path):\n",
        "        index, embeddings = create_vector_store(text_chunks, embedding_model)\n",
        "        faiss.write_index(index, vector_store_path)\n",
        "    else:\n",
        "        index = faiss.read_index(vector_store_path)\n",
        "\n",
        "    # Step 3: Load generation model and tokenizer\n",
        "    tokenizer = T5Tokenizer.from_pretrained(generation_model, legacy=True)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(generation_model)\n",
        "\n",
        "    # Step 4: Extract topics\n",
        "    combined_text = \" \".join(text_chunks)\n",
        "    main_topics = extract_topics_with_t5(combined_text, model, tokenizer)\n",
        "\n",
        "    # Step 5: Generate MCQs for each topic\n",
        "    questions = []\n",
        "    for idx, topic in enumerate(main_topics, start=1):\n",
        "        # Retrieve context from FAISS\n",
        "        context, scores = retrieve_context(topic, text_chunks, index, embedding_model)\n",
        "\n",
        "        # Safely retrieve the first score from the FAISS result\n",
        "        confidence_score = float(scores[0]) if len(scores) > 0 else None\n",
        "\n",
        "        # Generate MCQs using the context\n",
        "        mcqs = generate_mcqs_with_context(topic, \" \".join(context), model, tokenizer)\n",
        "        for i, mcq in enumerate(mcqs.split(\"\\n\\n\"), start=1):\n",
        "            if mcq.strip():\n",
        "                mcq_lines = mcq.split(\"\\n\")\n",
        "                question = {\n",
        "                    \"id\": f\"Q{idx}_{i}\",\n",
        "                    \"topic\": topic,\n",
        "                    \"type\": \"MCQ\",\n",
        "                    \"question\": mcq_lines[0].strip(),\n",
        "                    \"options\": [line.strip() for line in mcq_lines[1:5]],\n",
        "                    \"correct_answer\": mcq_lines[5].strip() if len(mcq_lines) > 5 else None,\n",
        "                    \"explanation\": mcq_lines[6].strip() if len(mcq_lines) > 6 else None,\n",
        "                    \"source\": {\n",
        "                        \"page_number\": None,  # Page numbers can be integrated with additional logic\n",
        "                        \"context\": \" \".join(context),\n",
        "                        \"confidence_score\": confidence_score,\n",
        "                    },\n",
        "                }\n",
        "                questions.append(question)\n",
        "\n",
        "    # Save metadata and questions to JSON\n",
        "    questions_data = {\n",
        "        \"metadata\": {\n",
        "            \"generated_at\": datetime.now().isoformat(),\n",
        "            \"total_questions\": len(questions),\n",
        "            \"book_title\": book_title,\n",
        "            \"generation_method\": \"RAG Pipeline\",\n",
        "            \"embedding_model\": embedding_model,\n",
        "            \"vector_store\": \"FAISS\",\n",
        "        },\n",
        "        \"questions\": questions,\n",
        "    }\n",
        "    with open(os.path.join(OUTPUT_DIR, \"questions_rag.json\"), \"w\") as f:\n",
        "        json.dump(questions_data, f, indent=4)\n",
        "\n",
        "    print(\"RAG pipeline completed successfully! Output saved to 'questions_rag.json'.\")\n",
        "\n",
        "# Run the RAG pipeline\n",
        "pdf_path = \"/content/Project Management.pdf\"  # Replace with your PDF path\n",
        "rag_pipeline(pdf_path, EMBEDDING_MODEL, GENERATION_MODEL, VECTOR_STORE)"
      ]
    }
  ]
}
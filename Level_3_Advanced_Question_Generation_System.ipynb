{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YoUrXxNsUx4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import faiss\n",
        "from datetime import datetime\n",
        "from PyPDF2 import PdfReader\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Set global parameters\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # Sentence embedding model\n",
        "GENERATION_MODEL = \"t5-base\"  # T5 model for question generation (public model)\n",
        "VECTOR_STORE = \"faiss_index\"\n",
        "OUTPUT_DIR = \"output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = []\n",
        "    for page in reader.pages:\n",
        "        text.append(page.extract_text())\n",
        "    return text\n",
        "\n",
        "# Function to create a vector store using FAISS\n",
        "def create_vector_store(text_chunks, embedding_model):\n",
        "    model = SentenceTransformer(embedding_model)\n",
        "    embeddings = model.encode(text_chunks, convert_to_tensor=False)\n",
        "    dimension = embeddings[0].shape[0]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(np.array(embeddings))\n",
        "    return index, embeddings\n",
        "\n",
        "# Function to retrieve relevant context from FAISS\n",
        "def retrieve_context(query, text_chunks, index, embedding_model, top_k=3):\n",
        "    model = SentenceTransformer(embedding_model)\n",
        "    query_embedding = model.encode([query], convert_to_tensor=False)\n",
        "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
        "    context = [text_chunks[i] for i in indices[0]]\n",
        "    return context, distances[0]\n",
        "\n",
        "# Function to fine-tune the model (stub for fine-tuning logic)\n",
        "def fine_tune_model(dataset_path, model_name, epochs=3):\n",
        "    # This is just a placeholder function. Fine-tuning would require a dataset and training loop.\n",
        "    # Fine-tuning on question-answer datasets can be done here.\n",
        "    pass\n",
        "\n",
        "# Function to generate MCQs using the model\n",
        "def generate_mcqs_with_context(topic, context, model, tokenizer, num_questions=15):\n",
        "    prompt = (\n",
        "        f\"Generate {num_questions} multiple-choice questions based on the topic '{topic}' \"\n",
        "        f\"using the following context:\\n{context}\\n\"\n",
        "        \"Provide each question with 4 options (A, B, C, D), specify the correct answer, and give a detailed explanation.\"\n",
        "    )\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=1024, num_beams=4, early_stopping=True)\n",
        "    mcqs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return mcqs\n",
        "\n",
        "# Function to generate open-ended questions\n",
        "def generate_open_ended_questions_with_context(topic, context, model, tokenizer, num_questions=5):\n",
        "    prompt = (\n",
        "        f\"Generate {num_questions} open-ended questions based on the topic '{topic}' \"\n",
        "        f\"using the following context:\\n{context}\\n\"\n",
        "        \"Provide each question with a model answer and key points.\"\n",
        "    )\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=1024, num_beams=4, early_stopping=True)\n",
        "    open_ended_questions = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return open_ended_questions\n",
        "\n",
        "# Main RAG pipeline\n",
        "def rag_pipeline(pdf_path, embedding_model, generation_model, vector_store_path):\n",
        "    book_title = \"Project Management Professional Guide\"\n",
        "\n",
        "    # Step 1: Extract text from PDF\n",
        "    pages = extract_text_from_pdf(pdf_path)\n",
        "    text_chunks = [page.strip() for page in pages if page.strip()]\n",
        "\n",
        "    # Step 2: Create FAISS vector store\n",
        "    if not os.path.exists(vector_store_path):\n",
        "        index, embeddings = create_vector_store(text_chunks, embedding_model)\n",
        "        faiss.write_index(index, vector_store_path)\n",
        "    else:\n",
        "        index = faiss.read_index(vector_store_path)\n",
        "\n",
        "    # Step 3: Load generation model and tokenizer\n",
        "    tokenizer = T5Tokenizer.from_pretrained(generation_model)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(generation_model)\n",
        "\n",
        "    # Step 4: Extract topics (For this example, we are using predefined topics)\n",
        "    main_topics = [\"Project Initiation\", \"Project Planning\", \"Project Execution\", \"Project Monitoring and Control\", \"Project Closure\"]\n",
        "\n",
        "    # Step 5: Generate MCQs and Open-Ended Questions for each topic\n",
        "    questions = {\n",
        "        \"mcq\": [],\n",
        "        \"open_ended\": []\n",
        "    }\n",
        "\n",
        "    for idx, topic in enumerate(main_topics, start=1):\n",
        "        # Retrieve context from FAISS\n",
        "        context, scores = retrieve_context(topic, text_chunks, index, embedding_model)\n",
        "\n",
        "        # Generate MCQs using the context\n",
        "        mcqs = generate_mcqs_with_context(topic, \" \".join(context), model, tokenizer)\n",
        "        for i, mcq in enumerate(mcqs.split(\"\\n\\n\"), start=1):\n",
        "            if mcq.strip():\n",
        "                mcq_lines = mcq.split(\"\\n\")\n",
        "                questions[\"mcq\"].append({\n",
        "                    \"id\": f\"MCQ-{idx}_{i}\",\n",
        "                    \"topic\": topic,\n",
        "                    \"type\": \"MCQ\",\n",
        "                    \"question\": mcq_lines[0].strip(),\n",
        "                    \"options\": [line.strip() for line in mcq_lines[1:5]],\n",
        "                    \"correct_answer\": mcq_lines[5].strip() if len(mcq_lines) > 5 else None,\n",
        "                    \"explanation\": mcq_lines[6].strip() if len(mcq_lines) > 6 else None,\n",
        "                    \"source\": {\n",
        "                        \"page_number\": None,\n",
        "                        \"confidence_score\": float(scores[0]) if len(scores) > 0 else None\n",
        "                    }\n",
        "                })\n",
        "\n",
        "        # Generate Open-Ended Questions using the context\n",
        "        open_ended = generate_open_ended_questions_with_context(topic, \" \".join(context), model, tokenizer)\n",
        "        for i, oe in enumerate(open_ended.split(\"\\n\\n\"), start=1):\n",
        "            if oe.strip():\n",
        "                oe_lines = oe.split(\"\\n\")\n",
        "                # Safely access the lines, checking if they exist\n",
        "                question = oe_lines[0].strip() if len(oe_lines) > 0 else None\n",
        "                model_answer = oe_lines[1].strip() if len(oe_lines) > 1 else None\n",
        "                key_points = oe_lines[2].strip().split(\", \") if len(oe_lines) > 2 else []\n",
        "\n",
        "                questions[\"open_ended\"].append({\n",
        "                    \"id\": f\"OE-{idx}_{i}\",\n",
        "                    \"topic\": topic,\n",
        "                    \"type\": \"open_ended\",\n",
        "                    \"question\": question,\n",
        "                    \"model_answer\": model_answer,\n",
        "                    \"key_points\": key_points,\n",
        "                    \"source\": {\n",
        "                        \"page_number\": None,\n",
        "                        \"context\": \" \".join(context),\n",
        "                        \"confidence_score\": float(scores[0]) if len(scores) > 0 else None\n",
        "                    }\n",
        "                })\n",
        "\n",
        "    # Save metadata and questions to JSON\n",
        "    questions_data = {\n",
        "        \"metadata\": {\n",
        "            \"generated_at\": datetime.now().isoformat(),\n",
        "            \"total_questions\": len(questions[\"mcq\"]) + len(questions[\"open_ended\"]),\n",
        "            \"book_title\": book_title,\n",
        "            \"model_info\": {\n",
        "                \"base_model\": generation_model,\n",
        "                \"fine_tuning_method\": \"PEFT\",\n",
        "                \"training_epochs\": 3\n",
        "            }\n",
        "        },\n",
        "        \"questions\": questions\n",
        "    }\n",
        "    with open(os.path.join(OUTPUT_DIR, \"questions_advanced.json\"), \"w\") as f:\n",
        "        json.dump(questions_data, f, indent=4)\n",
        "\n",
        "    print(\"Advanced question generation completed successfully! Output saved to 'questions_advanced.json'.\")\n",
        "\n",
        "# Run the RAG pipeline\n",
        "pdf_path = \"/content/Project Management.pdf\"  # Replace with your PDF path\n",
        "rag_pipeline(pdf_path, EMBEDDING_MODEL, GENERATION_MODEL, VECTOR_STORE)\n",
        "\n"
      ]
    }
  ]
}